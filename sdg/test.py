import os

from .storage.dataset import Dataset, DataType, Datadir, copy_dataset

from .event import global_message_queue, EventType, EventResponse
from .data_operator.operator import OperatorMeta
from .cost_evaluation import OperatorData,OperatorExecutor,refresh_operator_costs,flatten_secondary_metrics,operator_to_metrics
from .task.task import Task

from .data_insights_identify import sort_metrics
from openai import OpenAI
from datetime import datetime
import json
# LOG_FILE_PATH = "./operator_selection_log.jsonl"
def generate_negative_correlation_weights(result: dict) -> dict:
    """
    æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœç”Ÿæˆè´Ÿç›¸å…³æƒé‡ã€‚
    åˆ†æ•°è¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼Œå¹¶å½’ä¸€åŒ–åˆ° 0-100 èŒƒå›´ã€‚
    """
    import numpy as np

    # è·å–æ‰€æœ‰å¾—åˆ†å¹¶åå‘å¤„ç†
    scores = np.array(list(result.values()), dtype=np.float64)
    reversed_scores = 100 - scores  # åˆ†æ•°ä½ => å·®è·å¤§ => æƒé‡é«˜

    # å½’ä¸€åŒ–åˆ° 0-100
    min_val = reversed_scores.min()
    max_val = reversed_scores.max()
    if max_val == min_val:
        normalized_weights = np.ones_like(reversed_scores) * 50  # æ‰€æœ‰å€¼ç›¸ç­‰æ—¶ç»Ÿä¸€è®¾ä¸º50
    else:
        normalized_weights = 100 * (reversed_scores - min_val) / (max_val - min_val)

    # ç”Ÿæˆå­—å…¸å½¢å¼
    sorted_total_weights = {
        key: float(f"{weight:.4f}")  # ä¿ç•™å°æ•°ç‚¹å 4 ä½
        for key, weight in zip(result.keys(), normalized_weights)
    }

    # æŒ‰æƒé‡é™åºæ’åˆ—ï¼ˆå¯é€‰ï¼‰
    sorted_total_weights = dict(
        sorted(sorted_total_weights.items(), key=lambda item: item[1], reverse=True)
    )

    return sorted_total_weights

# å›ºå®šä½æ—¶é—´æˆ³ï¼Œåªç”Ÿæˆä¸€æ¬¡
timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
log_path = f"operator_selection_log_{timestamp_str}.json"
def log_iteration(sorted_total_weights, selected_operator, result, is_initial=False):
    # timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    # log_path = f"operator_selection_log_{timestamp_str}.json"

    log_entry = {
        "timestamp": datetime.now().isoformat(),
    }

    if is_initial:
        log_entry["initial_result"] = result
    else:
        log_entry["selected_operator"] = selected_operator
        log_entry["result"] = result
        log_entry["sorted_total_weights"] = sorted_total_weights

    with open(log_path, "a", encoding="utf-8") as f:
        f.write("\n" + "=" * 80 + "\n")  # åˆ†éš”çº¿ï¼Œä¾¿äºäººå·¥æŸ¥çœ‹
        f.write(json.dumps(log_entry, indent=2, ensure_ascii=False))  # ä¸­æ–‡ & ç¼©è¿›
        f.write("\n")


def describe_data(datadir: Datadir):
    dir_path = datadir.data_path
    count = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])
    data_type = datadir.data_type.value
    global_message_queue.put(EventResponse(EventType.REASONING, f'{data_type} data in {dir_path} has {count} files!'))


def describe_metadata(metadata_path: str):
    with open(metadata_path, 'r') as f:
        lines = f.readlines()
    global_message_queue.put(
        EventResponse(EventType.REASONING, f'multimodal dataset contains {len(lines) - 1} data pairs!'))


def test_cost():
    #æ„å»ºæ•°æ®é›†
    registry = OperatorMeta.get_registry()
    # print(registry)

    code_dir = Datadir('echart-code-sample-negative', DataType.CODE)

    describe_data(code_dir)
    image_dir = Datadir('echart-image-sample-negative', DataType.IMAGE)
    describe_data(image_dir)
    data_set = Dataset([code_dir, image_dir], 'echart-sample-negative.metadata', 'key_configurations.md')


    #åˆæ¬¡æ•°æ®è´¨é‡è¯„ä¼°
    result = data_set.evaluate_image_code_quality()
    print(result)
    # ğŸ’¾ è®°å½•åˆå§‹è´¨é‡è¯„ä¼°
    log_iteration(None, None, result, is_initial=True)
    #å°†è´¨é‡è¯„ä¼°ç»“æœè½¬æ¢æˆé¶ç‚¹å‘ç°æœŸæœ›çš„å½¢å¼
    result=flatten_secondary_metrics(result)
    # print(result)

    #è¿›è¡Œé¶ç‚¹å‘ç°ï¼Œè·å–é¶ç‚¹æƒé‡
    client = OpenAI(api_key="sk-3955d8823efd4f2483897446b91a7ffb", base_url="https://api.deepseek.com")
    sorted_metrics, sorted_total_weights = sort_metrics(client=client, code_quality_analysis=result,
                                                        llm_weight=0.7)
    # sorted_total_weights=generate_negative_correlation_weights(result)
    # print(sorted_metrics)
    # print(sorted_total_weights)
    # exit(0)

    # ç¬¬ä¸€æ¬¡æ„å»ºå¯é€‰ç®—å­æ± 
    operator_pool = []
    # éå†æ³¨å†Œè¡¨ä¸­æ‰€æœ‰ç®—å­
    for cls in registry.values():
        instance = cls()
        cost_info = instance.get_cost(data_set)
        name = cost_info["name"]

        # é»˜è®¤æƒé‡ä¸º 0
        weight = 0.0

        # æŸ¥æ‰¾è¯¥ç®—å­å¯¹åº”çš„æŒ‡æ ‡
        metric_list = operator_to_metrics.get(name, [])

        # ç´¯åŠ è¯¥ç®—å­å¯¹åº”æŒ‡æ ‡çš„æƒé‡
        for metric in metric_list:
            weight += sorted_total_weights.get(metric, 0.0)

        # æ„å»º OperatorData å¯¹è±¡
        op_data = OperatorData(
            name=name,
            ti=cost_info["ti"],
            ri=cost_info["ri"],
            ci=cost_info["ci"],
            wi=weight,
            type=cost_info["type"]
        )
        operator_pool.append(op_data)
    print(operator_pool)


    #åˆå§‹åŒ–ä»£ä»·è¯„ä¼°å™¨
    executor = OperatorExecutor(strategy='cost', t_limit=100, c_limit=10)
    while True:
        #é€‰ä¸­ä¸‹ä¸€ä¸ªæœ€åº”è¯¥æ‰§è¡Œçš„ç®—å­
        op = executor.choose_operator(operator_pool)
        if not op:
            print("\nğŸ’¡ å·²è¾¾åˆ°æ—¶é—´/æˆæœ¬é™åˆ¶ï¼Œç»“æŸé€‰æ‹©ã€‚")
            break
        #æ›´æ–°ä»£ä»·è¯„ä¼°å™¨
        metrics = executor.compute_metrics(op)
        executor.t_used += metrics["Ti"]
        executor.c_used += metrics["Ci"]
        executor.total_quality += metrics["Qi"]
        executor.execution_log.append(op.name)

        print(f"âœ… é€‰æ‹©ç®—å­ï¼š{op.name}")
        print(f"   â†’ Ti={metrics['Ti']:.2f}, Ci={metrics['Ci']:.5f}, Qi={metrics['Qi']:.2f}, Ri={metrics['Ri']:.2f}")
        print(f"   â†’ ç´¯è®¡æ—¶é—´ï¼š{executor.t_used:.2f} / {executor.t_limit}")
        print(f"   â†’ ç´¯è®¡æˆæœ¬ï¼š{executor.c_used:.5f} / {executor.c_limit}\n")


        # æ‰§è¡Œå¯¹åº”çš„ç®—å­ï¼Œæˆ‘è¿™é‡Œè°ƒè¯•æœ‰äº›å›°éš¾
        task = Task(
            [
                # é…ç½®é¡¹ä¿®æ­£
                registry[op.name](),

            ],
            data_set
        )
        result=task.run()
        print(result)
        # print("ç»“æœ1")
        data_set = task.final_dataset
        # operator_pool.remove(op)#åˆ å»è¯¥ç®—å­ï¼ˆä¹Ÿå¯ä¸åˆ ï¼‰ï¼Œåˆ å»è¡¨ç¤ºæ‰§è¡Œè¿‡çš„ä¸ä¼šè¢«å†æ¬¡æ‰§è¡Œï¼Œä¸åˆ åˆ™è¡¨ç¤ºå¯èƒ½åç»­è¿˜ä¼šå†æ¬¡è°ƒç”¨è¯¥ç®—å­

        #é‡æ–°è¿›è¡Œè´¨é‡è¯„ä¼°å¹¶æ›´æ–°ç®—å­åº“çš„å„ä¸ªå‚æ•°
        # result = data_set.evaluate_image_code_quality()
        # print(result)
        # print("ç»“æœ2")
        # exit(0)

        result = flatten_secondary_metrics(result)
        sorted_metrics, sorted_total_weights = sort_metrics(client=client, code_quality_analysis=result,
                                                            llm_weight=0.7)
        # sorted_total_weights = generate_negative_correlation_weights(result)
        # ğŸ’¾ è®°å½•æœ¬è½®é€‰æ‹©çš„æŒ‡æ ‡ä¸ç®—å­
        log_iteration(sorted_total_weights, op.name, result)#ç”¨äºè®°å½•
        operator_pool=refresh_operator_costs(registry, operator_pool, data_set,sorted_total_weights=sorted_total_weights,operator_to_metrics=operator_to_metrics)
        print(operator_pool)

    # exit(0)



if __name__ == '__main__':
    test_cost()